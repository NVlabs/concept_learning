robot_camera_pose = np.array([0.0,-2.8, 0.3,0.707,0.0,0.0,0.707]); t = as_float_array(from_euler_angles(-90.0 * 0.01745, 90.0 * 0.01745, 90 * 0.01745));robot_camera_pose[3:] = np.array([t[1], t[2], t[3], t[0]]);world_instance.camera_handle = spawn_camera(world_instance, env_ptr, 60, 640, 480, robot_camera_pose);camera_data = observe_camera(world_instance, env_ptr);plt.imshow(camera_data["color"]);plt.show()

    <material>
      <texture filename="model_normalized.mtl"/>
    </material>


  num_queries = args.concept_model.split("_")[-1][:-3]
  query_str = num_queries if num_queries.isnumeric() else ""
  self.save_dir = parent_dir + "/data/concept_shapenet/" + "{}_learned{}/".format(args.concept, query_str)


example = self.examples[0]; f= h5py.File(self.data_dir + "/input/" + example + ".hdf5", 'r'); depth = png_to_numpy(f["depth"][()]).astype(np.float32); mask = png_to_numpy(f["mask"][()]).astype(np.uint16); proj_matrix = f["camera_intrinsics"]["proj_matrix"][()]; view_matrix = f["camera_intrinsics"]["view_matrix"][()]; depth_max = f["camera_intrinsics"]["depth_max"][()]; depth_min = f["camera_intrinsics"]["depth_min"][()]; f= h5py.File(self.data_dir + "/{}/".format(self.label_folder) + example + ".hdf5", 'r'); label = f["label"][()]; depth = depth * (depth_max - depth_min) / 65535.0 + depth_min; camera_data = {'depth':depth, 'mask':mask, 'proj_matrix':proj_matrix, 'view_matrix':view_matrix}; camera_data = get_pointcloud_from_depth(camera_data)


import open3d as o3d; pcd = o3d.geometry.PointCloud(); pcd.points = o3d.utility.Vector3dVector(pts.cpu().detach().numpy()[:,:3]); o3d.visualization.draw_geometries([pcd])

import open3d as o3d; pcd = o3d.geometry.PointCloud(); pcd.points = o3d.utility.Vector3dVector(camera_data['pc']); mesh_frame = open3d.geometry.TriangleMesh.create_coordinate_frame(size=0.6, origin=[0,0,0]); o3d.visualization.draw_geometries([pcd, mesh_frame])


import open3d as o3d; pcd = o3d.geometry.PointCloud(); pcd.points = o3d.utility.Vector3dVector(points.T[:,:3]); mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.6, origin=[0,0,0]); o3d.visualization.draw_geometries([pcd, mesh_frame])

import open3d as o3d; pcd = o3d.geometry.PointCloud(); pcd.points = o3d.utility.Vector3dVector(pcs_old[i][:,:3]); o3d.visualization.draw_geometries([pcd])

for index in range(len(self.examples)): print(index); example = self.examples[index]; f= h5py.File(self.data_dir + "/input/" + example + ".hdf5", 'r'); depth = png_to_numpy(f["depth"][()]).astype(np.float32); mask = png_to_numpy(f["mask"][()]).astype(np.uint16); proj_matrix = f["camera_intrinsics"]["proj_matrix"][()]; view_matrix = f["camera_intrinsics"]["view_matrix"][()]; depth_max = f["camera_intrinsics"]["depth_max"][()]; depth_min = f["camera_intrinsics"]["depth_min"][()]; f.close(); f= h5py.File(self.data_dir + "/{}/".format(self.label_folder) + example + ".hdf5", 'r'); label = f["label"][()]; f.close();depth = depth * (depth_max - depth_min) / 65535.0 + depth_min; camera_data = {'depth':depth, 'mask':mask, 'proj_matrix':proj_matrix, 'view_matrix':view_matrix}; camera_data = get_pointcloud_from_depth(camera_data); anchor_pts = camera_data["pc"][camera_data["pc_seg"]==SegLabel.ANCHOR.value]; thing= np.mean(anchor_pts, axis=0)

obj_idxs = set()
for obj in upright_objs:
    for i, urdf in enumerat(obj_urdf_files):
        if obj in urdf:
            print(i)
    idxes = np.argwhere(obj in obj_urdf_files)
    print(idxes)
    obj_idxs.add(idx) for idx in idxes


return torch.cat((rot, pose[:,:3].unsqueeze(2)), axis=2)

moved_mean = torch.mean(movingpts_new[best_idx][:,:3], axis=0)
anchor_mean = torch.mean(pts[best_idx,pts[0, :, 4]==1,:3], axis=0)
length = torch.norm(moved_mean - anchor_mean)
print("PC length: ",length.item())

991, 948, 663, 952, 544

obj_idxs = set()
front_objs = ["CanOpener", "Fork", "Fruit_7", "Hammer", "Knife", "Mug", "Pan_a", "Scissors", "Spoon", "Stapler", "Teapot"]
lr_objs = ["CanOpener", "Fork", "Fruit_7", "Knife", "Mug", "Pan_a", "Spoon", "Stapler", "Teapot"]
on_objs = ["Book", "Bowl", "Calculator", "Box", "Pan", "Plate"]
towards_objs = ["CanOpener", "Fork", "Fruit_7", "Hammer", "Knife", "Mug", "Pan_a", "Scissors", "Spoon", "Stapler", "Teapot"]

for obj in front_objs:
    for i, urdf in enumerate(obj_urdf_files):
        if obj in urdf:
            print(i)
            obj_idxs.add(i)



mining_errors = {}
for mining in [0, 250, 500, 1000]:
    args.mining = mining
    objective_errors = []
    for objective in ["random", "confusion", "all", "allscheduled", "confrand", "allrand"]:
        args.objective = objective
        active_generator.reset_model()

        if args.passive_samples > 0:
            # Warmstart the model.
            active_generator.retrain(data_filename, label_filename)

        errors = active_generator.collect_data(args.concept, N_queries=args.active_samples, objective=args.objective,\
                                               warmstart=args.warmstart, mining=args.mining)
        objective_errors.append(errors)
    print(objective_errors)
    mining_errors[mining] = objective_errors
print(mining_errors)
active_generator.kill_instance()


xyz = np.repeat(np.array([0,0,0]).reshape((1,-1)), pts_center.shape[0], axis=0)
quat = np.repeat(np.array([1,0,0,0]).reshape((1,-1)), pts_center.shape[0], axis=0)


movingpts=pc_old[0,pc_old[0, :, 3]==1,:3]
movedpts=pc_new[0,pc_new[0, :, 3]==1,:3]
moving_center = torch.mean(movingpts,axis=0)
moved_center = torch.mean(movedpts,axis=0)


R = tra3d.quaternion_to_matrix(T[:,3:])
moving_center = torch.mean(pc_old[0,pc_old[0, :, 3]==1,:3],axis=0).unsqueeze(0).unsqueeze(0)
rawstate_pt = old_rawstate[:3].unsqueeze(0)
movingpts_rot = torch.matmul(rawstate_pt-moving_center, torch.transpose(R, 1, 2))
movingpts_trans = torch.add(movingpts_rot+moving_center, T[:,:3].unsqueeze(1))
pose_new = movingpts_trans.squeeze()
new_rawstate = torch.cat((pose_new, old_rawstate[7:])).detach().numpy()

moving_center = torch.mean(old_state[old_state[:, 3]==1,:3],axis=0).unsqueeze(0).to(device)
rawstate = copy.deepcopy(old_rawstate).unsqueeze(0).to(device)
rawstate[:,:3] -= moving_center
new_rawstate = transform_rawstate(rawstate, T.unsqueeze(0)).float()
new_rawstate[:,:3] += moving_center
new_rawstate = new_rawstate.squeeze()


idxes = np.hstack((moving_idx, anchor_idx)); pc_old = state[idxes].to(device).repeat(batch_size, 1, 1); pc_new = move_points(pc_old, T); 

movingpc_new = pc_new[:, pc_new[0, :, 3]==1, :]; movingpc_old = pc_old[:, pc_old[0, :, 3]==1, :]

idx=0; new_pts = torch.cat((movingpc_new[idx], state[notmoving_idx].to(device)), dim=0); new_pts = torch.cat((new_pts, torch.zeros((new_pts.shape[0],1)).to(device)),dim=1); new_pts = new_pts.cpu().detach().numpy(); old_pts = torch.cat((movingpc_old[idx], state[notmoving_idx].to(device)), dim=0); old_pts = torch.cat((old_pts, torch.zeros((old_pts.shape[0],1)).to(device)),dim=1); old_pts = old_pts.cpu().detach().numpy()

show_pcs_with_frame(old_pts, pts[0][:3].cpu().detach().numpy())



moving_center = torch.mean(pc_old[:,pc_old[0, :, 3]==1,:3], axis=1); old_rawstate = copy.deepcopy(rawstate).unsqueeze(0).to(device); old_rawstate[:,:3] -= moving_center[idx]; new_rawstate = transform_rawstate(old_rawstate, T[idx].unsqueeze(0)).float(); new_rawstate[:,:3] += moving_center[idx]; new_rawstate = new_rawstate.squeeze().cpu().detach().numpy()

show_pcs_with_frame(new_pts, new_rawstate[:3])

self.hdf5cacher_data = Hdf5Cacher(self.data_path, 'r'); self.hdf5cacher_label = Hdf5Cacher(self.label_path, 'r')

index=0; example = self.examples[index]; data = self.hdf5cacher_data.__getitem__(example);label = self.hdf5cacher_label.__getitem__(example);depth = png_to_numpy(data["depth"]).astype(np.float32);mask = png_to_numpy(data["mask"]).astype(np.uint16);proj_matrix = data["proj_matrix"];view_matrix = data["view_matrix"];depth_max = data["depth_max"];depth_min = data["depth_min"];label = label["label"];depth = depth * (depth_max - depth_min) / 65535.0 + depth_min;camera_data = {'depth':depth, 'mask':mask, 'proj_matrix':proj_matrix, 'view_matrix':view_matrix};camera_data = get_pointcloud_from_depth(camera_data)



30 CanOpener
31 CanOpener
32 CanOpener
33 CanOpener
34 CanOpener
35 CanOpener
54 Fork
55 Fork
56 Fork
57 Fork
58 Fork
59 Fork
63 Fruit_7
64 Fruit_7
65 Fruit_7
66 Hammer
67 Hammer
68 Hammer
69 Hammer
70 Hammer
71 Hammer
72 Knife
73 Knife
74 Knife
75 Knife
76 Knife
77 Knife
84 Mug
85 Mug
86 Mug
87 Mug
88 Mug
89 Mug
90 Pan_a
91 Pan_a
92 Pan_a
102 Scissors
103 Scissors
104 Scissors
105 Scissors
106 Scissors
107 Scissors
114 Spoon
115 Spoon
116 Spoon
117 Spoon
118 Spoon
119 Spoon
120 Stapler
121 Stapler
122 Stapler
123 Stapler
124 Stapler
125 Stapler
126 Teapot
127 Teapot
128 Teapot
129 Teapot
130 Teapot
131 Teapot



0 Bottle
1 Bottle
2 Bottle
3 Bottle
4 Bottle
5 Bottle
12 Bottle
13 Bottle
14 Bottle
15 Bottle
16 Bottle
17 Bottle
132 Bottle
133 Bottle
134 Bottle
135 Bottle
136 Bottle
137 Bottle
18 Bowl
19 Bowl
20 Bowl
21 Bowl
22 Bowl
23 Bowl
36 Candle
37 Candle
38 Candle
39 Candle
40 Candle
41 Candle
48 Cup
49 Cup
50 Cup
51 Cup
52 Cup
53 Cup
78 MilkCarton
79 MilkCarton
80 MilkCarton
81 MilkCarton
82 MilkCarton
83 MilkCarton
84 Mug
85 Mug
86 Mug
87 Mug
88 Mug
89 Mug
90 Pan
91 Pan
92 Pan
93 Pan
94 Pan
95 Pan
96 Plate
97 Plate
98 Plate
99 Plate
100 Plate
101 Plate
126 Teapot
127 Teapot
128 Teapot
129 Teapot
130 Teapot
131 Teapot


    elif args.concept in ["aligned"]:
        obj_idxes = [0, 1, 2, 3, 4, 5, 132, 133, 134, 135, 136, 137, 12, 13, 14, 15, 16, 17, \
                     36, 37, 38, 39, 40, 41, 48, 49, 50, 51, 52, 53, 78, 79, 80, 81, 82, 83, \
                     84, 85, 86, 87, 88, 89]
        obj_idxes = [0, 1, 2, 3, 4, 5, 132, 133, 134, 135, 136, 137, 12, 13, 14, 15, 16, 17, \
                     36, 37, 38, 39, 40, 41, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
        obj_idxes = [18, 19, 20, 21, 22, 23, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, \
                     126, 127, 128, 129, 130, 131]


    elif args.concept in ["front", "front45"]:
        obj_idxes = [30, 31, 32, 57, 66, 67, 68, 69, 70, 71, 75, 76, 77, 84, 85, 86, 87, 88, 89, 90, \
                     91, 92, 102, 103, 104, 105, 106, 107, 117, 118, 123, 124, 126, 127, 128, 129, 130, 131]
        obj_idxes = [30, 57, 66, 67, 68, 69, 70, 71, 75, 84, 85, 86, 87, 88, 89, 90, \
                     91, 92, 102, 103, 104, 105, 106, 107, 117, 123, 126, 127, 128, 129, 130, 131]
        obj_idxes = [32, 57, 66, 67, 68, 69, 70, 71, 90, 91, 92, 102, 103, 104, 105, 106, 107,\
                     117, 123, 126, 127, 128, 129, 130, 131]
        obj_idxes = [66, 67, 68, 69, 70, 71, 90, 91, 92, 102, 103, 104, 105, 106, 107, 126, 127, 128, 129, 130, 131]
        obj_idxes = [66, 67, 68, 69, 70, 71, 90, 91, 92, 126, 127, 128, 129, 130, 131]



# Load object meshes for visualization purposes.
parent_dir = os.path.abspath(os.path.dirname(os.path.abspath(__file__)) + "/../..")
asset_root = os.path.abspath(parent_dir + "/data/shapenet_objects/")
obj_mesh_files = sorted(glob.glob("{}/meshes/*.obj".format(asset_root)))
obj_meshes = [trimesh.load(obj_mesh_file) for obj_mesh_file in obj_mesh_files]


objs = dataset.examples[idx].split("_")
moving_mesh = obj_meshes[int(objs[0])]
anchor_mesh = obj_meshes[int(objs[1])]
moving_T = pose_to_T(torch.tensor(old_rawstate[:7]).unsqueeze(0))
anchor_T = pose_to_T(torch.tensor(old_rawstate[7:14]).unsqueeze(0))

obj1_quat = torch.tensor([old_rawstate[6], old_rawstate[3], old_rawstate[4], old_rawstate[5]]).unsqueeze(0)
obj2_quat = torch.tensor([old_rawstate[13], old_rawstate[10], old_rawstate[11], old_rawstate[12]]).unsqueeze(0)
moving_vertices = tra3d.quaternion_apply(torch.tensor([0.707,0,0.707,0.0]).unsqueeze(0), torch.tensor(moving_mesh.vertices))
anchor_vertices = tra3d.quaternion_apply(torch.tensor([0.707,-0.707,0.0,0.0]).unsqueeze(0), torch.tensor(anchor_mesh.vertices))

moving_vertices = trimesh.transform_points(moving_mesh.vertices, moving_T.squeeze())
anchor_vertices = trimesh.transform_points(anchor_mesh.vertices, anchor_T.squeeze())
moving_faces = moving_mesh.faces
anchor_faces = anchor_mesh.faces
show_pcs_with_mesh(old_state, moving_vertices, moving_faces, shadow=True)